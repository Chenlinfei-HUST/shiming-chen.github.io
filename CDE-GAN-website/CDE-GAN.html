

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head><TITLE>CDE-GAN(Shiming Chen)</TITLE>
<META content="text/html; charset=UTF-8" http-equiv=Content-Type>

<META content=IE=Edge,chrome=1 http-equiv=X-UA-Compatible>
<LINK rel=stylesheet href="css/style.css" media=screen>
<STYLE type=text/css>
@media Print
{
#STTBimg {
    DISPLAY: none
}
#STTBimg2 {
    DISPLAY: none
}
    }
</STYLE>
<style type="text/css">
      body{background:#FFFFFF;text-align:center;}
      div{width:970px;margin:0 auto;background:#fff;text-align:left;}

      
.STYLE2 {font-size: 18px}
.STYLE6 {font-size: 24pt}
.STYLE8 {color: #CC0000}
.STYLE14 {color: #000000; font-size: 17px; font-weight: bold; }
.STYLE15 {color: #000000}
.STYLE16 {font-size: 22px}
.STYLE20 {color: #FF0000; font-weight: bold; }
.STYLE22 {color: #0000FF}
</style>


<META name=GENERATOR content="MSHTML 9.00.8112.16441"></head>
<body class=" hasGoogleVoiceExt" screen_capture_injected="true">

<div class="container"> 
      <br><br>
	  <h1 class="STYLE6" style="TEXT-ALIGN: center">CDE-GAN: Cooperative Dual Evolution Based Generative Adversarial Network</H1>
	  <br>
	  <span class="STYLE2"><br>
      </span>
     <p style='text-align: center;'> 
       <span style="font-size: 18px"><a class='urllink' href='https://shiming-chen.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Shiming Chen</strong></font></a><sup>1</sup>,
        <a class='urllink' href='' title='' rel='nofollow'><font color="#0000FF"><strong>Wenjie Wang</strong></font></a><sup>1</sup>,
		<a class='urllink' href='' title='' rel='nofollow'><font color="#0000FF"><strong>Beihao Xia</strong></font></a><sup>1</sup>,
	<a class='urllink' href='http://bmal.hust.edu.cn/info/1005/1091.htm' title='' rel='nofollow'><font color="#0000FF"><strong>Xinge You</strong></font></a><sup>1</sup>,
       <a class='urllink' href='http://bmal.hust.edu.cn/info/1005/1092.htm' title='' rel='nofollow'><font color="#0000FF"><strong>Qinmu Peng</strong></font></a><sup>1</sup>,
	 <a class='urllink' href='https://czh513.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Zehong Cao</strong></font></a><sup>2</sup>,
	 and <a class='urllink' href='https://www.researchgate.net/profile/Weiping_Ding2' title='' rel='nofollow'><font color="#0000FF"><strong>Weiping Ding</strong></font></a><sup>3</sup>
	    </span><br>
	     
     </p>
      <p class='vspace STYLE2' style='text-align: center;'>
        <sup>1</sup>Huazhong University of Science and Technology (HUST), China  </p>
      <p class='vspace STYLE2' style='text-align: center;'>
        <sup>2</sup>University of Tasmania (UTAS), Australia  &nbsp &nbsp  <sup>3</sup>Nantong University (NTU), China </p>
      <p class='vspace STYLE2' style='text-align: center;'>
          <font color="#0000FF" font-style='italic' font-family='NimbusMonL-Regu'>{shimingchen, wangwj54, xbh_hust, youxg, pengqinmu}@hust.edu.cn  &nbsp    zehong.cao@utas.edu.au  &nbsp  ding.wp@ntu.edu.cn </font> </p>

      <div class=vspace> </div>
	  <div class=line> </div>
	  <br>
	  
      <h2 align="center">Abstract</h2>
      <P align="justify">Generative adversarial networks (GANs) have been a popular deep generative model for real-word applications.  Despite many recent efforts on GANs have been contributed, 
	  however, mode collapse and instability of GANs are still open problems caused by their adversarial optimization difficulties. In this paper, motivated by the cooperative co-evolutionary algorithm,
	  we propose a <i>Cooperative Dual Evolution based Generative Adversarial Network</i> (<strong>CDE-GAN</strong>) to circumvent these drawbacks. In essence, CDE-GAN  incorporates dual evolution with respect to
	  generator(s) and discriminators into a unified evolutionary adversarial framework, thus it exploits the complementary properties and injects dual mutation diversity into training to steadily 
	  diversify the estimated density in capturing multi-modes, and to improve generative performance. Specifically, CDE-GAN decomposes the complex adversarial optimization problem into two subproblems
	  (generation and discrimination), and each subproblem is solved with a separated subpopulation (<i>E-Generators</i> and <i>E-Discriminators</i>), evolved by an individual evolutionary algorithm.
	  Additionally, to keep the balance between E-Generators and E-Discriminators, we proposed a <i>Soft Mechanism</i> to cooperate them to conduct effective adversarial training. Extensive experiments
	  on one synthetic dataset and three real-world benchmark image datasets, demonstrate that the proposed CDE-GAN achieves the competitive and superior performance in generating good quality and
	  diverse samples over baselines. The code and more generated results are available at our project homepage <a class='urllink' href=' https://shiming-chen.github.io/CDE-GAN-website/CDE-GAN.html' title=''
	  rel='nofollow'><font color="#0000FF"><strong> https://shiming-chen.github.io/CDE-GAN-website/CDE-GAN.html</strong></font></a>. </P>
	  <div class=line> </div>
	  <p></p>
      <h2>Model pipeline </h2>
      
	  <div class="div-table-col-12" align='center'>
                <center><img src="figures/CDEGAN-pipeline-new.jpg" width="950" height="350"></center>
				<p></p>
				<P align="justify"><strong>Figure:</strong>The pipeline of CDE-GAN. In brief, CDE-GAN decomposes the complex adversarial optimization problem into two subproblems (generation and discrimination), 
				and each subproblem is solved with a separated subpopulation (i.e., E-Generatorsand E-Discriminators), evolved by an individual evolutionary algorithm
				(including individual Variations, Evaluations and Selections). The best offspring of E-Generators and E-Discriminator are served as new parents
				to produce the next generation's individuals (i.e., children). Furthermore, a Soft Mechanism is proposed to cooperate E-Generators and E-Discriminators to conduct effective adversarial training.</P>
				
				
	  </div>
	 <div class=line> </div>
	  <p></p>
      <h2>Material </h2>
	 <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="div-table-row">
			  	<div class="div-table-col-3" align='center' id="img-large" >
                <center><a class=urllink title="" href="figures/CDE-GAN.pdf" rel=nofollow><img src="figures/CDE-GAN-page1.png" width="150" height="150"
				class="layered-paper-big"></br></br></br><h3><font color="#0000FF"><strong>Paper</strong></font></h3></a></center>
				</div>
				<div class="div-table-col-3" align='center' id="img-large1" style="margin-left:150px">
                <center><a class=urllink title="" href="" rel=nofollow><img src="figures/github-figure.jpg" 
				width="150" height="150"></br></br></br><h3><font color="#0000FF"><strong>Code</strong><br></font>&nbsp<font color="#FF0000"><strong>(Coming soon)</strong></font></h3></a></center>
				</div>
	  </div>	
	  </div>  
	   </form>
      
          <div class=vspace></div>
	
	  <div>
	   
	    <p>If you wish to use our code, please cite the following paper : </p>

	  
	   <div class=div-table-reference>
        <strong>CDE-GAN: Cooperative Dual Evolution Based Generative Adversarial Network</strong><br>
        Shiming Chen, Wenjie Wang, Beihao Xiao, Xinge You, Qinmu Peng, Zehong Cao, Weiping Ding<br>
        <em>arXiv preprint arXiv: 2008.09388, </em> 2020<br>		
      </div>	   
  </div>

      <div class=vspace></div>     
	   
	   <div class=line> </div>
	  <p></p>
	  
	  
	  
	  <h2>Evaluation</h2>
	  
	  
	  <p class="STYLE15">There are three different benchmark datasets used for evaluating our method, i.e.,  
	  <a class=urllink title="" href="http://www.cs.toronto.edu/~kriz/cifar.html" rel=nofollow><font size="4.2" font-weight="bold" color="#0000FF">CIFAR-10</font></a>,
	  <a class=urllink title="" href="https://github.com/fyu/lsun" rel=nofollow><font size="4.2" font-weight="bold" color="#0000FF">LSUN-Bedrooms</font></a>,
	  <!--<a class=urllink title="" href="" rel=nofollow><font size="4.2" font-weight="bold" color="#0000FF">Stacked-MNIST</font></a>,-->
	  and <a class=urllink title="" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html " rel=nofollow><font size="4.2" color="#0000FF">CelebA</font></a>.
						Some of these generated images are used in paper, in which supports better vision quality. Indeed, we also evaluate our method on synthesis dataset. </p>
	 
	  
	  <div class=line> </div>
	  
	  <h3 class="STYLE16" id="exp1">Experiment 1: Experiments on CIFAR-10 for hyper-parameters analysis</h3>
	  <p class="STYLE15">Experiments on the CIFAR-10 dataset for hyper-parameters analysis. (a) Inception score evaluation for different CDE-GANs with various balance factor
	  &gamma;={1,0.5,0.1,0.01}. (b) Inception score evaluation for different CDE-GANs with various numbers of discriminators I={1,2,4,8}.</p>
	  
	  
	  
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="div-table-col-12" align='center'>
                <center><img src="figures/experiment1.png" width="950" height="320"></center>
				
		</div>
	   </div>  
	   </form>
	  
	  
	  
	  <div class=line> </div>
	  
	  <h3 class="STYLE16">Experiment 2: Generative Performance Evaluation </h3>
	  <h4 class="STYLE16">2.1 Generated results on Synthetic dataset </h4>
	  <p class="STYLE15">Dynamic results of Gaussian kernel estimation over generator iteration for different GANs. For each pair of images, the left one is data distribution 
	  (real data is represented in blue, generated data is represented in red), and the right one is KDE plots of the generated data corresponded to its left generated data. From
	  top to bottom, the rows are the results of original GAN,  NS-GAN, LSGAN, E-GAN, and <strong>CDE-GAN (Ours)</strong>.</p>
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	    <div class="div-table-col-12" align='center'>
                <center><img src="figures/difgan-toy-mlp3.jpg" width="960" height="600"></center>
				<center><h3>(a) Gaussian kernel estimation with MLP of 3 layers</h3></center>
				
				
		</div>
		<div class="div-table-col-12" align='center'>
              <center><img src="figures/difgan-toy-mlp4.jpg" width="960" height="600"></center>
				<center><h3>(b) Gaussian kernel estimation with MLP of 4 layers</h3></center>  
		</div>
				
				
	   </div>   
	   </form>
	   
	   <h4 class="STYLE16">2.2 Generated results on real-world datasets </h4>
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="div-table-col-12" align='center'>
                <center><img src="figures/CelebA.png" width="960" height="960"></center>
				<center><h3>(a) CelebA</h3></center>	
		</div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="figures/CIFAR.png" width="960" height="960"></center>
				<center><h3>(b) CIFAR-10</h3></center>	
		</div>
		<div class="div-table-col-12" align='center'>
                <center><img src="figures/LSUN.png" width="960" height="960"></center>
				<center><h3>(c) LSUN-Bedrooms</h3></center>	
		</div>
		
		<!--
	    <div class="div-table-col-12" align='center'>
                <center><img src="figures/CIFAR-1.png" width="300" height="300">
				&nbsp &nbsp <img src="figures/CIFAR-2.png" width="300" height="300">
				&nbsp &nbsp <img src="figures/CIFAR-3.png" width="300" height="300"></center>
				<center><h5>(a) CIFAR-10</h5></center>	
		</div>
		
        <div class="div-table-col-12" align='center'>
                <center><img src="figures/LSUN-1.png" width="300" height="300">
				&nbsp &nbsp <img src="figures/LSUN-2.png" width="300" height="300">
				&nbsp &nbsp <img src="figures/LSUN-3.png" width="300" height="300"></center>
				<center><h5>(b) LSUN-Bedrooms</h5></center>
		</div>
		<div class="div-table-col-12" align='center'>
                <center><img src="figures/CelebA-1.png" width="300" height="300">
				&nbsp &nbsp <img src="figures/CelebA-2.png" width="300" height="300">
				&nbsp &nbsp <img src="figures/CelebA-3.png" width="300" height="300"></center>
				<center><h5>(c) CelebA</h5></center>
		</div>
		-->
	
	   </div>   
	   </form>
	  
	
	<div class=line> </div>
	<p> </p>
	<p> </p>
	<h3 class="STYLE16">Experiment 3: Comparisons with state-of-the-art methods </h3>
	  <h4 class="STYLE16">3.1 Quantitative comparison on CIFAR-10 dataset </h4>
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	    <div class="div-table-col-12" align='center'>
                <center><img src="figures/Table1.png" width="430" height="400"> &nbsp &nbsp &nbsp <img src="figures/Table2.png" width="430" height="408"></center>
	
				
				
		</div>
				
	   </div>   
	   </form>
	   
	   <h4 class="STYLE16">3.2 Qualitative comparison on CelebA </h4>
	  <form id="exp_texture">
	  <p>Samples generated by different methods on various natural image datasets. The samples generated by different methods are provided by the original literatures, 
	  i.e., MAD-GAN [23], Lipizzaner [24], Mustangs [27], Stabilizing-GAN [10], and acGAN [19].
	   <div class="div-table" align='center'>     
	    <div class="div-table-col-12" align='center'>
                <center><img src="figures/Image-compared.png" width="900" height="600">
					
		</div>
	
	   </div>   
	   </form>	  
	   
	
	  
	  
	 
	   

	 
	    <!--<center><img src="../../images/animation_hummingbird.gif" width=50% alt="" align="middle"></center> -->

	  <div class=line> </div>
      <!-- <div class=line-seg> </div> -->
      <p></p>
      <h2>Acknowledgement</h2>

      <p align="justify">This work was supported in part by the National Natural Science Foundation of China~(61571205 and 61772220), 
	      the Key Program for International S\&T Cooperation Projects of China~(2016YFE0121200), the Special Projects for Technology Innovation of Hubei Province~(2018ACA135), 
		  the Key Science and Technology Innovation Program of Hubei Province~(2017AAA017), the Natural Science Foundation of Hubei Province~(2018CFB691), 
		  fund from Science, Technology and Innovation Commission of Shenzhen Municipality~(JCYJ20180305180637611, JCYJ20180305180804836 and JSGG20180507182030600).
		  We thank Dr. 
	      <a class='urllink' href='https://www.sydney.edu.au/engineering/about/our-people/academic-staff/chaoyue-wang.html' title='' rel='nofollow'><font color="#0000FF"><strong> Chaoyue Wang</strong></font></a>
	      for his assistance with coding and theoretical suggestions.</p>
      <div class=line> </div>
      
	   <p></p>
	   <h2>Reference</h2>

      <p align="justify">[1] Chaoyue Wang et al. "Evolutionary generative adversarial networks." <em>IEEE Transactions on Evolutionary Computation (TEVC)</em>, 2019.</p>
	  <p align="justify">[2] Isabela Albuquerque et al. "Multi-objective training of Generative Adversarial Networks with multiple discriminators." <em>In ICML</em>, 2019.</p>
	  <p align="justify">[3] Jamal Toutouh et al. "Spatial evolutionary generative adversarial networks." <em>In GECCO</em>, 2019.</p>
	  <p align="justify">[4] Tom Schmiedlechner et al. "Towards distributed coevolutionary gans." <em>In AAAI</em>, 2018.</p>
      <p align="justify">[5] Ishan Durugkar  et al. "Generative multi-adversarial networks." <em>In ICLR</em>, 2017.</p>
	  <p align="justify">[6] Ian J. Goodfellow  et al. "Generative adversarial nets." <em>In NIPS</em>, 2014.</p>
	  
      <div class=line> </div>
      <h4> <center><a class='urllink' href='./CDE-GAN.html' title='' rel='nofollow'><font color="#0000FF">Top</font></a> </center></h4>
</div>
</body></html>
