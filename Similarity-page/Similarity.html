

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head><TITLE>Similarity-DT: Kernel similarity embedding for Dynamic Texture Synthesis</TITLE>
<META content="text/html; charset=UTF-8" http-equiv=Content-Type>

<META content=IE=Edge,chrome=1 http-equiv=X-UA-Compatible>
<LINK rel=stylesheet href="css/style.css" media=screen>
<STYLE type=text/css>
@media Print
{
#STTBimg {
    DISPLAY: none
}
#STTBimg2 {
    DISPLAY: none
}
    }
</STYLE>
<style type="text/css">
      body{background:#FFFFFF;text-align:center;}
      div{width:970px;margin:0 auto;background:#fff;text-align:left;}

      
.STYLE2 {font-size: 18px}
.STYLE6 {font-size: 24pt}
.STYLE8 {color: #CC0000}
.STYLE14 {color: #000000; font-size: 17px; font-weight: bold; }
.STYLE15 {color: #000000}
.STYLE16 {font-size: 22px}
.STYLE20 {color: #FF0000; font-weight: bold; }
.STYLE22 {color: #0000FF}
</style>


<META name=GENERATOR content="MSHTML 9.00.8112.16441"></head>
<body class=" hasGoogleVoiceExt" screen_capture_injected="true">

<div class="container"> 
      <br><br>
	  <h1 class="STYLE6" style="TEXT-ALIGN: center">Similarity-DT: Kernel Similarity Embedding for Dynamic Texture Synthesis</H1>
	  <br>
	  <span class="STYLE2"><br>
      </span>
     <p style='text-align: center;'> 
       <span style="font-size: 18px"><a class='urllink' href='https://shiming-chen.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Shiming Chen</strong></font></a><sup>1</sup>,
        <a class='urllink' href='' title='' rel='nofollow'><font color="#0000FF"><strong>Peng Zhang</strong></font></a><sup>1</sup>,
	<a class='urllink' href='http://bmal.hust.edu.cn/info/1005/1091.htm' title='' rel='nofollow'><font color="#0000FF"><strong>Xinge You</strong></font></a><sup>1</sup>,
       <a class='urllink' href='http://bmal.hust.edu.cn/info/1005/1092.htm' title='' rel='nofollow'><font color="#0000FF"><strong>Qinmu Peng</strong></font></a><sup>1</sup>,
        <a class='urllink' href='http://www.hqumadm.com/' title='' rel='nofollow'><font color="#0000FF"><strong>Xin Liu</strong></font></a><sup>2</sup>,
	 <a class='urllink' href='https://czh513.github.io/' title='' rel='nofollow'><font color="#0000FF"><strong>Zehong Cao</strong></font></a><sup>3</sup>,
         and <a class='urllink' href='https://sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html' title='' rel='nofollow'><font color="#0000FF"><strong>Dacheng Tao</strong></font></a><sup>4</sup>
	    </span><br>
	     
     </p>
      <p class='vspace STYLE2' style='text-align: center;'>
        <sup>1</sup>Huazhong University of Science and Technology (HUST), China &nbsp &nbsp <sup>2</sup>Huaqiao University, China</p>
      <p class='vspace STYLE2' style='text-align: center;'>
        <sup>3</sup>University of Tasmania (UTAS), Australia &nbsp &nbsp <sup>4</sup> University of Sydney (USYD), Australia  </p>
      <p class='vspace STYLE2' style='text-align: center;'>
          <font color="#0000FF" font-style='italic' font-family='NimbusMonL-Regu'>{shimingchen, zp_zhg, youxg, pengqinmu}@hust.edu.cn  &nbsp   xliu@hqu.edu.cn  &nbsp  zehong.cao@utas.edu.au  &nbsp  dacheng.tao@sydney.edu.au </font> </p>

      <div class=vspace> </div>
	  <div class=line> </div>
	  <br>
	  
      <h2 align="center">Abstract</h2>
      <P align="justify">Dynamic texture (DT) exhibits statistical stationarity in the spatial domain and stochastic repetitiveness
	      in the temporal dimension, indicating that different frames of DT possess high similarity correlation. However, 
	      there are no DT synthesis methods to consider the similarity prior for representing DT instead, which can explicitly 
	      capture the homogeneous and heterogeneous correlation between different frames of DT. In this paper, we propose a novel
	      DT synthesis method (named Similarity-DT), which embeds the similarity prior into the representation of DT. 
	      Specifically, we first raise two hypotheses: the content of texture video frames varies over time-to-time, 
	      while the more closed frames should be more similar; the transition between frame-to-frame could be modeled as
	      a linear or nonlinear function to capture the similarity correlation. Then, our proposed Similarity-DT integrates
	      kernel learning and extreme learning machine (ELM) into a powerful unified synthesis model to learn kernel similarity
	      embedding to represent the spatial-temporal transition among frame-to-frame of DTs. Extensive experiments on DT videos
	      collected from internet and two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex, demonstrate that the 
	      learned kernel similarity embedding effectively exhibits the discriminative representation for DTs. 
	      Hence our method is capable of preserving long-term temporal continuity of the synthesized DT sequences with
	      excellent sustainability and generalization. We also show that our method effectively generates realistic DT videos
	      with fast speed and low computation, compared with the state-of-the-art approaches. </P>
	  <div class=line> </div>
	  <p></p>
       <h2>Material </h2>
      
	  
	 <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="div-table-row">
			  	<div class="div-table-col-3" align='center' id="img-large" >
                <center><a class=urllink title="" href="https://arxiv.org/abs/1911.04254" rel=nofollow><img src="Videos-gif/arxiv_paper_image.png" width="150" height="150" class="layered-paper-big"></br></br></br><h3><font color="#0000FF"><strong>Paper</strong></font></h3></a></center>
				</div>
				<div class="div-table-col-3" align='center' id="img-large1" style="margin-left:150px">
                <center><a class=urllink title="" href="https://github.com/shiming-chen/Similariy-DT" rel=nofollow><img src="Videos-gif/github-figure.jpg" width="150" height="150"></br></br></br><h3><font color="#0000FF"><strong>Code</strong></font></h3></a></center>
				</div>
	  </div>	
	  </div>  
	   </form>
      
          <div class=vspace></div>
	
	  <div>
	   
	    <p>If you wish to use our code, please cite the following paper : </p>

	  
	   <div class=div-table-reference>
        <strong>Similarity-DT: Kernel Similarity Embedding for Dynamic Texture Synthesis</strong><br>
        Shiming Chen, Peng Zhang, Xinge You, Qinmu Peng, Xin Liu, Zehong Cao, Dacheng Tao<br>
        <em>arXiv preprint arXiv: 1911.04254, </em> 2019<br>		
      </div>	   
  </div>

      <div class=vspace></div>     
	   
	   <div class=line> </div>
	  <p></p>
	  <h2>Evaluation</h2>
	  <p class="STYLE15">There are two different benchmark datasets used for evaluating our method: <a class=urllink title="" href="http://www.cc.gatech.edu/cpl/projects/graphcuttextures" rel=nofollow><font size="4.2" font-weight="bold" color="#0000FF">Gatech Graphcut Textures</font></a> dataset and <a class=urllink title="" href="http://projects.cwi.nl/dyntex/database.html" rel=nofollow><font size="4.2" color="#0000FF">Dyntex</font></a> dataset.
						Some of these generated dynamic texture videos are used in paper, in which supports better vision quality. Indeed, we also evaluate our method on others DTs in-the-wild. If you are interested in the quantitative evaluation results <span class="STYLE20">(PSNR, SSIM)</span>, please take it from our paper.</p>
	  <cneter><p>
		   <span class="STYLE20">Notably, it will take some time for loading this page because videos are long-term, please wait for minutes. </span></p></center>
	  
	  <div class=line> </div>
	  
	  <h3 class="STYLE16" id="exp1">Experiment 1: Synthesizing DTs using various kernel fuctions</h3>
	  <p class="STYLE15">In each example, the first one is the observed video, the others are the generated videos synthesized by <strong>Similarity-DT</strong> using different kernel function (<span class="STYLE20">left-to-right</span>: <strong>Linear kernel, Rational Quadratic kernel, Polynomial kernel,
						Multiquadric kernel, Sigmoid kernel, Gaussian kernel</strong>) </p>
	  
	  
	  
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            <h5>Rotating wind ornament</h5>
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-parameters/Different-kernel1-small1.gif" width="960" height="100">
				</div></center>
				
	   </div>  
	   </form>
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            <h5>windmill</h5>
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-parameters/Different-kernel2-small1.gif" width="960" height="100">
				</div></center>   
	   </div>  
	   </form>
	  
	  
	  
	  <div class=line> </div>
	  
	  <h3 class="STYLE16">Experiment 2: Synthesizing high-fidelity, long-term DTs (Sustainability Analysis) </h3>
	  <p class="STYLE15">In each row, the first three are the observed videos with 200 frames, the other three are the synthesized videos generated by <strong>Similarity-DT</strong> with 1000 frames. 
						Here, we display 18 dynamic texture sequences of 6 classes (top-to-bottom: bulb, elevator, flowers swaying in the current, rotating wind ornament, water wave, windmill).</p>
	  
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            <h5></h5>
          </center></div>
          			      		  
       </div>
	    <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Sustainability/sustainability-all.gif" width="700" height="465">
				</div></center>
				
	   </div>  
	    
	   </form>
	  

	  
	
	<div class=line> </div>
	  
	  <h3 class="STYLE16">Experiment 3: Synthesizing DTs using transferred model (Gneralization Analysis)</h3>
	  <p class="STYLE15">In each group, the first row displays the observed videos (<span class="STYLE20">used for testing</span>). As for other rows, the first video is the observed video (<span class="STYLE20">used for training</span>), the others are the synthesized videos corresponding to the first row.</p>
	  
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
			<h5>Cows</h5>
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" style='margin-right:-20.3%'>
                <img src="Videos-gif/In-the-wild/cow-origin-small1.gif" width="720" height="100">
				</div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/cow1-cow-small1.gif" width="864" height="100"></center>	
				</div> 
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/cow2-cow-small1.gif" width="864" height="100"></center>	
				</div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/cow3-cow-small1.gif" width="864" height="100"></center>	
				</div> 
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/cow4-cow-small1.gif" width="864" height="100"></center>	
				</div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/cow5-cow-small1.gif" width="864" height="100"></center>	
				</div>
	   </div>  
	   </form>
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
          <h5>Tigers</h5>
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" style='margin-right:-7.4%'>
                <center><img src="Videos-gif/In-the-wild/tiger-origin-small1.gif" width="288" height="100"></center>	
				</div> 
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/tiger1-tiger-small1.gif" width="432" height="100"></center>	
				</div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/In-the-wild/tiger2-tiger-small1.gif" width="432" height="100"></center>	
				</div>
	   </div>  
	   </form>
	
	
	
	
	
	
	  <div class=line> </div>
	  <h3 class="STYLE16" id="exp1">Experiment 4: Vision quality comparison with baseline methods</h3>
	  <p class="STYLE15">The first group displays observed DT videos, the others are the generated videos synthesized by different methods 
	  (<span class="STYLE20">top-to-bottom:</span> <a class='urllink' href='https://ryersonvisionlab.github.io/two-stream-projpage/' title='' rel='nofollow'><font color="#0000FF"><strong>Two-Stream</strong></font></a> [<font color="00FF00">4</font>], <a class='urllink' href='http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html' title='' rel='nofollow'><font color="#0000FF"><strong>STGCN</strong></font></a> [<font color="00FF00">3</font>], <a class='urllink' href='http://www.stat.ucla.edu/~jxie/DynamicGenerator/DynamicGenerator.html' title='' rel='nofollow'><font color="#0000FF"><strong>DG</strong></font></a> [<font color="00FF00">2</font>],
						ours(<strong>Similarity-DT</strong>)) </p>
	  
	  
	  
	  
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-methods/Observed-small1.gif" width="900" height="100"></center>
				</div>
	   </div>  
	   </form> 
	   
	   <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-methods/CVPR2018.gif" width="900" height="100"></center>	
				</div>	
	   </div>  
	   </form> 
	   
	   <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-methods/PAMI2019.gif" width="900" height="100"></center>	
				</div>			
	   </div>  
	   </form>
	 
	  <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-methods/AAAI-short.gif" width="900" height="100"></center>	
				</div>			
	   </div>  
	   </form>
	   
	   <form id="exp_texture">
	   <div class="div-table" align='center'>     
	   <div class="headRow" align='center'>
          <div class="div-table-col-12"><center>
            
          </center></div>
          			      		  
       </div>
	   <div class="div-table-col-12" align='center'>
                <center><img src="Videos-gif/Different-methods/Similarity-small1.gif" width="900" height="100"></center>	
				</div>			
	   </div>  
	   </form>
	   
	
	  
	  
	 
	   

	 
	    <!--<center><img src="../../images/animation_hummingbird.gif" width=50% alt="" align="middle"></center> -->

	  <div class=line> </div>
      <!-- <div class=line-seg> </div> -->
      <p></p>
      <h2>Acknowledgement</h2>

      <p align="justify">This work was supported in part by the National Natural Science Foundation of China~(61571205 and 61772220), 
	      the Key Program for International S\&T Cooperation Projects of China~(2016YFE0121200). We thank Dr. 
	      <a class='urllink' href='http://www.stat.ucla.edu/~jxie/' title='' rel='nofollow'><font color="#0000FF"><strong> Jianwen Xie</strong></font></a>
	      for his suggestions.</p>
      <div class=line> </div>
      
	   <p></p>
	   <h2>Reference</h2>

      <p align="justify">[1] Xinge You et al. "Kernel Learning for Dynamic Texture Synthesis." <em>IEEE Transactions on Image Processing (TIP)</em>, 2016.</p>
	  <p align="justify">[2] Jianwen Xie  et al. "Learning Dynamic Generator Model by Alternating Back-Propagation through Time." <em>In AAAI</em>, 2019.</p>
      <p align="justify">[3] Jianwen Xie  et al. "Energy-based spatial-temporal generative convNet." <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2019.</p>
      <p align="justify">[4] Matthew Tesfaldet et al. "Two-stream convolutional networks for dynamic texture synthesis." <em>In CVPR</em>, 2018.</p>
	  <p align="justify">[5] Gatys Leon A. et al. "Texture synthesis using convolutional neural networks." <em>In NeurIPS</em>, 2015.</p>
	  <p align="justify">[6] Gianfranco Doretto et al. "Dynamic textures." <em>International Journal of Computer Vision (IJCV)</em>, 2003.</p>
      <div class=line> </div>
      <h4> <center><a class='urllink' href='./STGConvNet.html' title='' rel='nofollow'><font color="#0000FF">Top</font></a> </center></h4>
</div>
</body></html>
